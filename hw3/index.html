<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Rachel Hu, Philip Ye</div>

		<br>

		Link to webpage:  <a href="https://cal-cs184-student.github.io/hw-webpages-skibidirizz/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-skibidirizz/hw3/index.html</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-jabroni-boi">https://github.com/cal-cs184-student/sp25-hw3-jabroni-boi</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>Cornell Boxes with Bunnies</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h4>Ray Generation Walkthrough</h4>
		<p>A camera captures the world into a 2D image, or "image space." Ray generation is essentially a way to estimate/recover the original real-world coordinates of a given point in this image space.</p>
		<p>The first step is to convert image space coordinates, (x, y), into camera space coordinates, (x, y, z). This is essentially done by fixing a Z value in camera space, then shifting and scaling the image (x, y) coordinates into their corresponding camera space coordinates.
			The z dimension here is intuitively the direction that the camera is looking at, called the viewing direction. We fix Z so as to put all image space coordinates in a plane in 3D.</p>
		<p>The ray from our camera (the origin in camera space) to this calculated location on the camera plane, is our desired camera space ray.</p>
		<p>Then, we use the camera-to-world conversion matrix to convert the camera space ray into world space. This can be thought of as a change of basis conversion. The new origin of the ray is therefore the camera's position in world space.</p>
		<p>Anything beyond a certain threshold set by min_t and max_t are considered out of frame from the camera's viewpoint. We also normalize the world space vector so as to keep consistent calculations across different rays and directions.</p>
		<p>We can then use this calculation to determine many things. For example, we used this, alongside Monte Carlo Integration, to estimate the radiance in this ray direction.</p>
		
		<h4>Primitive Intersection Walkthrough</h4>
		<p>You have a ray, and a primitive object, such as a triangle or sphere, and you want to check if and where they intersect. You can achieve this simply by setting the equations of these objects equal to each other to see if there exists a point at which they are equal. With rays specifically, this means calculating the t in the ray equation where it intersects the primitive object, and checking if the t-value obtained is valid or not.</p>
		<p>t being valid essentially means its within our field of view. If the intersection point is beyond our t-bounds, we cannot see the object. Every time we get a new, valid t, we set our maximum t to that t value. This makes sense intuitively, becomes it essentially means "once our ray hits this object, we can't see any object behind it."</p>
		<p>Checking these intersection points is used to correctly render objects in front of others.</p>
		
		<h4>Triangle Intersection Algorithm</h4>
		<p>
			To check whether a ray intersection a triangle, we first checked if the ray intersected the plane the triangle was on, by deriving the plane equation from the triangle's coordinates, then setting these equations equal to each other to find the t ("time") at which the ray intersects the plane.
			If it did intersect, we used a 3D point-in-triangle test to determine if the intersection point between the ray and the plane, lied inside the triangle.
			For that, we checked whether or not the point was on the correct side of each edge of the triangle. If it were on the correct / same side of all three edges, it means it is inside the triangle.
		</p>
		<p>
			Later on, we used the Moller Trumbore algorithm to get barycentric coordinates for our intersection point within the triangle, which we then used to interpolate the surface normal at the intersection point. This surface normal has many use cases, in this case namely for shading and lighting.
		</p>
		<h4>Example Images</h4>
		<p>Here's a few examples of the result of implementing ray generation and primitive intersection algorithms:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBempty.png" width="300px"/>
				  <figcaption>CBempty</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBspheres.png" width="300px"/>
				  <figcaption>CBspheres</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="banana.png" width="300px"/>
				  <figcaption>banana</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="building.png" width="300px"/>
				  <figcaption>building</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<!-- <p>Here is an example 2x2 gridlike structure using an HTML table. Each <b>tr</b> is a row and each <b>td</b> is a column in that row. You might find this useful for framing and showing your result images in an organized fashion.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
			  </tr>
			</table>
		</div> -->
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
			<p>
				In our BVH construction algorithm, we decided to recursively divide the list of primitives into a binary tree structure (per the spec), where each node contains a bounding box that encloses its child primitives. We'd like to organize the primitives spatially so that ray-primitive intersection tests can be performed efficiently, 
			by quickly culling (or "pruning") large sections of the scene that do not intersect the ray.
			</p>
			<p>
				We start out our algorithm with some given range of primitives (start to end) and checks whether the number of primitives is less than or equal to a user-specified max_leaf_size. If it happens to be so, we create a leaf node by storing that range of primitives and then return.
			However, if the number of primitives exceeds max_leaf_size, the algorithm must decide how to split the primitives into two subsets, and this is where our splitting heuristic comes in.
			</p>
			<p><b>The Heuristic: Surface Area Heuristic Cost</b></p>
			<p>
				
				<li>The heuristic used to determine which was the best is surface area heuristic cost (surface area of left box * num objects in left + surface area of right box * num objects in right)</li>
				
				<li>Created new std::vectors for left and right boxes. then made recursive call for both sides to continue BVH.</li>
				
				<li>throughout the entire internal node process, caught a few potential bad edge cases by defaulting the node back to the leaf node, namely if after splitting by best median centroid, one side is still empty (this would indicate that all objects had the same centroid on all axes).</li>
				</p>
			
			

		<h2>Part 3: Direct Illumination</h2>
		<p>
			In this part, we implemented 2 different types of direct lighting: direct lighting with uniform hemisphere sampling and direct lighting with importance sampling lights
		</p>

		<p>
			At a high level, Direct Lighting with <b>Uniform Hemisphere sampling</b> works by sampling directions uniformly over the hemisphere centered around the surface normal. We calculate the hit point of the ray using its origin and direction, as well 
			as the number of samples to use, and from here, we follow a couple of steps:
			<li>
				Sample a random direction using hemisphereSampler->get_sample()
			</li>
			<li>
				Transform said direction into our world coordinates, then we cast a shadow ray from the hit point in the sampled direction to check if it intersects with any object in the scene. 
				If it hits an object, we need to check whether that object emits light using bsdf->get_emission()	
			</li>
			<li>
				Evaluate the BSDF at the surface using the outgoing direction (w_out) and the sampled incoming direction
			</li>
			<li>
				From here, we just run the Monte Carlo algorithm and sum it up. We have f(p, wj -> wr), L(p, wj), cos(theta) and p(wj), so we can just plug it into our formula from Lecture 13!
			</li>
		</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<img src="mclol.png" width="300px"/>
		</div>
		

		<p>
			<b>Importance sampling</b> aims to improves the efficiency from uniform hemisphere sampling by sampling only directions that point toward actual lights in the scene. We use our knowledge about light sources to bias sampling toward more meaningful directions -> hence the name importance sampling!
		</p>
		<p>
			This time, we'll iterate through all the lights in the scene. If we have a delta light (point light), only sample one direction to the light. 
			If not, we'll have to sample ns_area_light times to better approximate the range of incoming directions.
		</p>
		<p>
			<li>
				For each light, we'll call light->sample_light. This gives us radiance of the light from w_i, and it writes out the direction w_i, distance to the light, and the associated sampling pdf
			</li>
			<li>
				Discard "bad" lights (invalid pdfs, or "shadow rays". These can be tested by casting a shadow ray and checking for other intersections with blocking objects.)
			</li>
			<li>
				Now run the Monte Carlo algorithm similar to Uniform Sampling. For area lights, we need to average the L_out produced by all the samples.
			</li>
		</p>


		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="bunny_64_32.png" width="300px"/>
				  <figcaption>Hemisphere Lighting</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="bunny_64_32_imp_sampling.png" width="300px"/>
				  <figcaption>Importance Sampling</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="dragon_screenshot_3-29_2-45-58.png" width="300px"/>
				  <figcaption>Hemisphere Lighting</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="dragon_screenshot_3-29_2-48-45.png" width="300px"/>
				  <figcaption>Importance Sampling</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<p>It's a little difficult to see at first, but looking at areas with higher shadows, you can tell that Importance Lighting causes more contrast, giving darker shadows and whiter whites. 
			By contrast, Uniform Hemisphere Lighting gives more equal, softer shadows across the rabbit. We can tell this is most likely due to the fact that Uniform Hemisphere Lighting does exactly what the name suggests - namely
			it uses the Monte Carlo algorithm to uniformly and randomly sample directions, hence the more equal distribution of light. By contrast, our Importance Lighting algorithm
			 takes into consideration point light sources as well as multiple bounce lights, so we have a more "accurate" and contrasting photo.
		</p>
		
		
		<h2>Part 4: Global Illumination</h2>
		<p>In this part, we implement indirect lighting through recursive path tracing. Since we want to trace inverse paths, we need to figure out where to go to next, and now, our next location can be a light or an object. 
			This helps us to account for the light that has bounced at least once off other surfaces before reaching an object!
		</p>
		<p>To do this, we use set a hard cap at max_ray_depth, and we can set the recursion limit in 2 ways: 1) Using a fix number N bounces and 2) Russian Roulette Termination</p>
		<p><b>Global Illumination with up to N Bounces of Light</b></p>
		<p>
			<li>
				The first thing to do is set our base cases. If our ray depth is <= 1, then we know we've hit a base case - return L_out.
			</li>
			<li>
				We also need to consider the direct lighting radiance, which happens if isAccumBounces is true or if we've hit the max depth.
			</li>
			<li>
				Now, we sample the BSDF to figure out the next direction to go to. We need to check if the new ray we spawned (the inverse path) escaped
				the scene without hitting anything, so we test for intersection. If there's is an environment left in the scene, we need to sample it!
			</li>
			<li>
				With our new ray and (possibly new) intersection, we need to recursively call the function with r.depth - 1 in our new ray. Once we have the recursive result, 
				we simply add it up in our Monte Carlo Estimator as usual.
			</li>
		</p>

		<p><b>Russian Roulette</b></p>
		<p>This is not all that different - in fact all we do is add a line of code which flips a coin with probability p of success. Upon success, we 
			terminate the ray by returning L_out. If not, we continue, but modify our monte carlo to be sample light to be scaled by 1 / (1-p). 
			This ensures that a ray which is able to consistently escape termination will not end up contributing as much light.
		</p>

		<div style="display: flex; justify-content: center; gap: 40px; margin-bottom: 20px;">
			<div style="text-align: center;">
			  <img src="bunny-1l-indirect.png" width="300px"/>
			  <figcaption>Bunny</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="sphere-1l-global.png" width="300px"/>
			  <figcaption>Spheres</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="empty-l1-indirect.png" width="300px"/>
			  <figcaption>Empty box</figcaption>
			</div>
		  </div>
		  <p>Comparing Indirect roulette and Direct lighting: </p>
		  <div style="display: flex; justify-content: center; gap: 40px; margin-bottom: 20px;">
			<div style="text-align: center;">
			  <img src="bunny-1l-indirect.png" width="300px"/>
			  <figcaption>Indirect (global)</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="bunny-direct.png" width="300px"/>
			  <figcaption>Direct</figcaption>
			</div>
		  </div>

		<p>Here are some images rendered at 1024 samples per pixels with the max ray depth set at 0-5 with isAccumBounces set to false:</p>
		<div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 40px; margin-bottom: 20px;">
			<div style="text-align: center;">
			  <img src="./noaccum/0m-bunny.png" width="200px"/>
			  <figcaption>m=0</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./noaccum/1m-bunny.png" width="200px"/>
			  <figcaption>m=1</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./noaccum/2m-bunny.png" width="200px"/>
			  <figcaption>m=2</figcaption>
			</div>
			<div style="text-align: center;">
				<img src="./noaccum/3m-bunny.png" width="200px"/>
				<figcaption>m=3</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./noaccum/4m-bunny.png" width="200px"/>
				<figcaption>m=4</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./noaccum/5m-bunny.png" width="200px"/>
				<figcaption>m=5</figcaption>
			  </div>
		  </div>
		  <p>We can see here that when m=2 or m=3, we have much softer shadows compared to rasterization. Furthermore, there's a subtle difference between m=2 and m=3, we can see the lighting is 
			  more evenly distributed when m=3, which is a general trend that happens as m increases. Also, we have more "color bleeding" as m increases, which is where the color of objects starts to bleed a little bit into the surfaces around them.
		  </p>

		  <p>And here are the same images rendered at 1024 samples per pixels with the max ray depth set at 0-5 with isAccumBounces set to true:</p>
		  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 40px; margin-bottom: 20px;">
			  <div style="text-align: center;">
				<img src="./accum/0m-acc-bunny.png" width="200px"/>
				<figcaption>m=0</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./accum/1m-acc-bunny.png" width="200px"/>
				<figcaption>m=1</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./accum/2m-acc-bunny.png" width="200px"/>
				<figcaption>m=2</figcaption>
			  </div>
			  <div style="text-align: center;">
				  <img src="./accum/3m-acc-bunny.png" width="200px"/>
				  <figcaption>m=3</figcaption>
				</div>
				<div style="text-align: center;">
				  <img src="./accum/4m-acc-bunny.png" width="200px"/>
				  <figcaption>m=4</figcaption>
				</div>
				<div style="text-align: center;">
				  <img src="./accum/5m-acc-bunny.png" width="200px"/>
				  <figcaption>m=5</figcaption>
				</div>
			</div>
		<p>As expected, setting isAccumBounces to true allows us to account for all the accumulated light up to max ray depth, resulting in far richer global illumination.</p>
		<p>Here are the same images with isAccumBounces set to true using Russian Roulette of p = 0.3 and m=32</p>
		<div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 40px; margin-bottom: 20px;">
			<div style="text-align: center;">
			  <img src="./rr/0rr-bunny.png" width="200px"/>
			  <figcaption>m=0</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./rr/1rr-bunny.png" width="200px"/>
			  <figcaption>m=1</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./rr/2rr-bunny.png" width="200px"/>
			  <figcaption>m=2</figcaption>
			</div>
			<div style="text-align: center;">
				<img src="./rr/3rr-bunny.png" width="200px"/>
				<figcaption>m=3</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./rr/4rr-bunny.png" width="200px"/>
				<figcaption>m=4</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./rr/5rr-bunny.png" width="200px"/>
				<figcaption>m=5</figcaption>
			  </div>
		  </div>
		<p>Finally, here's some pictures of the spheres with different pixel sample rates 1-1024, scaling by 2:</p>
		<div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 40px; margin-bottom: 20px;">
			<div style="text-align: center;">
			  <img src="./spheres/1ss-sphere.png" width="200px"/>
			  <figcaption>s=1</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./spheres/2ss-sphere.png" width="200px"/>
			  <figcaption>s=2</figcaption>
			</div>
			<div style="text-align: center;">
			  <img src="./spheres/4ss-sphere.png" width="200px"/>
			  <figcaption>s=4</figcaption>
			</div>
			<div style="text-align: center;">
				<img src="./spheres/8ss-sphere.png" width="200px"/>
				<figcaption>s=8</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./spheres/16ss-sphere.png" width="200px"/>
				<figcaption>s=16</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./spheres/32ss-spheres.png" width="200px"/>
				<figcaption>s=32</figcaption>
			  </div>

			  <div style="text-align: center;">
				<img src="./spheres/64ss-sphere.png" width="200px"/>
				<figcaption>s=64</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./spheres/128s-sphere.png" width="200px"/>
				<figcaption>s=128</figcaption>
			  </div>
			  <div style="text-align: center;">
				<img src="./spheres/256ss-sphere.png" width="200px"/>
				<figcaption>s=256</figcaption>
			  </div>
			  <div style="text-align: center;">
				  <img src="./spheres/1024ss-sphere.png" width="200px"/>
				  <figcaption>s=1024</figcaption>
				</div>
		  </div>
		
		  

		<h2>Part 5: Adaptive Sampling</h2>
		<p>In the Monte Carlo path tracer, we normally sample each pixel a fixed number of times provided by the -s flag. The problem is not every part of an image is equally complex - some pixels converge to a stable color with fewer samples
			while others need many more. We try to solve this by dynamically deciding how many samples each pixel needs:
		</p>
		<div>
			<li>
				We take a few samples of each pixel
			</li>
			<li>
				We then try to estimate how noisy this pixel currently is
			</li>
			<li>
				Once we've some threshold of "good enough", we can stop sampling
			</li>
			<li>
				Otherwise, keep sampling.
			</li>
		</div>

		<p>The idea of convergence is basically using a statistical test to see if the illuminance (brightness of a color) is "converged" or not. After each sample we
			compute illuminance and the sum of them / the sum of their squares -> this gives us the mean and variance which we use a statistical test to see if we converged or not.
		</p>
		<p>The way we implemented this was pretty 1-1 with the algorithm description - for each sample, we check the color sample given by the path tracer, and get its illuminance. We then update s1 and s2, our counters, and 
			increment our counter n. Every batch of samples (roughly 32) we will compute I, and if we've converged, we can stop taking samples for the pixel. If not, keep taking samples.
		</p>
		<p>Once we've converged, we compute the final averaged color of our pixel as the accumulated color / n, where n is how many samples we used. N also helps us generate our heatmap</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="sphere.png" width="300px"/>
				  <figcaption>Sphere</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="bunny.png" width="300px"/>
				  <figcaption>Bunny</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="sphere_rate.png" width="300px"/>
				  <figcaption>Sphere Sample Rate Heat Map</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="bunny_rate.png" width="300px"/>
				  <figcaption>Bunny Sample Rate Heat Map</figcaption>
				</td>
			  </tr>
			</table>
		</div>
	
		</div>
	</body>
</html>